<div align="center">

# üéôÔ∏è SimWhisper-Codec

### Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate Speech Coding

<p align="center">
  <img src="docs/assets/SimWhisper-Codec.png" width="80%" alt="SimWhisper-Codec Architecture">
</p>

<p>
  <a href="https://zhangxinwhut.github.io/SimWhisper-Codec/"><img src="https://img.shields.io/badge/üéß_Demo-Online-brightgreen" alt="Demo"></a>
  <a href="https://arxiv.org/pdf/2510.20504v2"><img src="https://img.shields.io/badge/Paper-Arxiv-red" alt="paper"></a>
  <a href="https://huggingface.co/xxx123456/SimWhisper_Codec"><img src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model%20Page-yellow" alt="Hugging Face"></a>
</p>

*A semantic-first speech codec that achieves superior performance through architectural simplification rather than complex supervision.*

</div>

---

## ‚ú® Highlights

- üöÄ **low Bitrate**: Only **1.1 kbps** at 16 kHz sampling rate
- üîä **High Quality Speech Reconstruction**: Achieving UTMOS 4.00 WER 2.75 (hubert-large-ls960-ft) sim 0.83 (wavlm_large_finetune) stoi 0.93 pesq-nb 3.29 pesq-wb 2.72 on librispeech-test-clean reconstruction (gt: WER 2.16 UTMOS 4.09)
- üåç **Strong Generalization**: Trained only on LibriSpeech, yet performs well on out-of-domain data (LJSpeech, THCHS30)
- üßä **Frozen Encoder**: No fine-tuning of Whisper encoder required
- ‚ö° **Simple & Efficient**: Architectural simplification over complex supervision

## üìä Performance

### In-Domain Evaluation (LibriSpeech test-clean)

| Model | Bitrate | WER ‚Üì | PESQ-NB ‚Üë | PESQ-WB ‚Üë | STOI ‚Üë | SIM ‚Üë | UTMOS ‚Üë |
|:------|:-------:|:-----:|:---------:|:---------:|:------:|:-----:|:-------:|
| XCodec2.0 | 0.8 kbps | 2.61 | 3.04 | 2.43 | 0.92 | 0.82 | **4.13** |
| XY-Tokenizer | 1.0 kbps | **2.46** | 3.10 | 2.50 | 0.92 | **0.85** | 4.03 |
| BigCodec | 1.04 kbps | 2.92 | 3.27 | 2.68 | **0.93** | 0.84 | 4.11 |
| **SimWhisper-Codec** | 1.1 kbps | 2.75 | **3.29** | **2.72** | **0.93** | 0.83 | 4.00 |

### Out-of-Domain Generalization

*Evaluation on Seed-TTS-Eval Dataset*
<table>
  <thead>
    <tr>
      <th align="center">Model</th>
      <th align="center">Bitrate</th>
      <th align="center" colspan="4">SEED-ZH</th>
      <th align="center" colspan="4">SEED-EN</th>
    </tr>
    <tr>
      <th></th>
      <th></th>
      <th align="center">PESQ-NB ‚Üë</th>
      <th align="center">PESQ-WB ‚Üë</th>
      <th align="center">SIM ‚Üë</th>
      <th align="center">STOI ‚Üë</th>
      <th align="center">PESQ-NB ‚Üë</th>
      <th align="center">PESQ-WB ‚Üë</th>
      <th align="center">SIM ‚Üë</th>
      <th align="center">STOI ‚Üë</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>XCodec2.0</td>
      <td align="center">0.8 kbps</td>
      <td align="center">2.69</td>
      <td align="center">2.10</td>
      <td align="center">0.81</td>
      <td align="center">0.89</td>
      <td align="center">2.57</td>
      <td align="center">2.01</td>
      <td align="center">0.78</td>
      <td align="center">0.89</td>
    </tr>
    <tr>
      <td>XY-Tokenizer</td>
      <td align="center">1.0 kbps</td>
      <td align="center">2.97</td>
      <td align="center">2.32</td>
      <td align="center"><b>0.88</b></td>
      <td align="center">0.90</td>
      <td align="center">2.75</td>
      <td align="center">2.18</td>
      <td align="center"><b>0.82</b></td>
      <td align="center">0.90</td>
    </tr>
    <tr>
      <td>BigCodec</td>
      <td align="center">1.04 kbps</td>
      <td align="center">2.88</td>
      <td align="center">2.26</td>
      <td align="center">0.80</td>
      <td align="center"><b>0.91</b></td>
      <td align="center">2.80</td>
      <td align="center">2.22</td>
      <td align="center">0.80</td>
      <td align="center"><b>0.91</b></td>
    </tr>
    <tr>
      <td><b>SimWhisper-Codec</b></td>
      <td align="center">1.1 kbps</td>
      <td align="center"><b>3.30</b></td>
      <td align="center"><b>2.38</b></td>
      <td align="center">0.82</td>
      <td align="center"><b>0.91</b></td>
      <td align="center"><b>2.88</b></td>
      <td align="center"><b>2.29</b></td>
      <td align="center">0.80</td>
      <td align="center"><b>0.91</b></td>
    </tr>
  </tbody>
</table>

| Dataset | Language | Samples | PESQ-WB ‚Üë | PESQ-NB ‚Üë | STOI ‚Üë |
|:--------|:--------:|:-------:|:---------:|:---------:|:------:|
| LJSpeech | English | 2,620 | 2.79 | 3.30 | 0.94 |
| THCHS-30 | Chinese | test set | 2.63 | 3.21 | 0.91 |

*SimWhisper-Codec is trained exclusively on LibriSpeech, demonstrating strong cross-lingual generalization.*

## üöÄ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/ZhangXinWhut/SimWhisper-Codec.git && cd SimWhisper-Codec

# Create and activate conda environment
conda create -n swcodec python=3.10 -y && conda activate swcodec

# Install dependencies
pip install -r requirements.txt
```

## Available Models üóÇÔ∏è

| Model Name | Hugging Face | Training Data |
|:----------:|:-------------:|:---------------:|
| SimWhisper-Codec | [ü§ó](https://huggingface.co/xxx123456/SimWhisper_Codec) | LibriSpeech |


### Download Model Weights

You need to download the SimWhisper-Codec model weights. You can find the weights in the [SimWhisper-Codec Hugging Face repository](https://huggingface.co/xxx123456/SimWhisper_Codec).

```bash
mkdir -p ./weights && huggingface-cli download xxx123456/SimWhisper_Codec SimWhisperCodec.pt --local-dir ./weights/
```

### Inference

```python
python inference.py --input_dir /path/to/LibriSpeech/test-clean
```

The reconstructed audio files will be available in the `output_wavs/` directory.

## üôè Acknowledgements

Our codebase builds upon the [XY-Tokenizer](https://github.com/gyt1145028706/XY-Tokenizer). We thank the authors for their excellent work.

## üìù Citation

If you find this work useful in your research, please cite our paper:

```
@misc{zhang2025speakingclearlysimplifiedwhisperbased,
      title={Speaking Clearly: A Simplified Whisper-Based Codec for Low-Bitrate Speech Coding}, 
      author={Xin Zhang and Lin Li and Xiangni Lu and Jianquan Liu and Kong Aik Lee},
      year={2025},
      eprint={2510.20504},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2510.20504}, 
}
```
## üìú License
This project is licensed under the Apache 2.0 License.
